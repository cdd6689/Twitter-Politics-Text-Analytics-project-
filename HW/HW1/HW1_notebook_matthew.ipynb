{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper\n",
    "First of all, don't run it!, it took me hours to scrape all the data (It should not have taken so long but I don't know why).\n",
    "\n",
    "Secondly, I scraped around 7500 most recent pieces of comment instead of 5000 because the most recent 5000 comments are contributed by 68 users. With 7500 comments, we can get opioins from 130 users, which diversifys our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#change directory where you put your chrome driver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome(\"chromedriver\", options=options)\n",
    "\n",
    "all_comments = pd.DataFrame(columns = ['Date','user_id', 'comments']) \n",
    "\n",
    "x = 284\n",
    "pages_scraped = 434\n",
    "driver.get('https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p{}'.format(x))\n",
    "\n",
    "while x <= pages_scraped:\n",
    "    print(\"page\",x)\n",
    "    \n",
    "    if x > 1:\n",
    "        driver.find_element_by_link_text(\"{}\".format(x)).click()\n",
    "        driver.implicitly_wait(3)\n",
    "    \n",
    "    ids = driver.find_elements_by_xpath(\"//*[contains(@id,'Comment_')]\")\n",
    "    comment_ids = []\n",
    "    for i in ids:\n",
    "        comment_ids.append(i.get_attribute('id'))\n",
    "\n",
    "    for d in comment_ids:\n",
    "\n",
    "        #Extract dates from for each user on a page\n",
    "        user_date = driver.find_elements_by_xpath('//*[@id=\"' + d +'\"]/div/div[2]/div[2]/span[1]/a/time')[0]\n",
    "        date = user_date.get_attribute('title')\n",
    "\n",
    "        #Extract user ids from each user on a page\n",
    "        userid_element = driver.find_elements_by_xpath('//*[@id=\"' + d +'\"]/div/div[2]/div[1]/span[1]/a[2]')[0]\n",
    "        userid = userid_element.text\n",
    "\n",
    "        #Extract Message for each user on a page\n",
    "        if driver.find_elements_by_xpath('//*[@id=\"' + d +'\"]/div/div[3]/div/div[1]/p'):\n",
    "            user_message = driver.find_elements_by_xpath('//*[@id=\"' + d +'\"]/div/div[3]/div/div[1]/p')[0]\n",
    "            comment = user_message.text\n",
    "        elif driver.find_elements_by_xpath('//*[@id=\"' + d +'\"]/div/div[3]/div/div[1]/ul/li'):\n",
    "            user_message = driver.find_elements_by_xpath('//*[@id=\"' + d +'\"]/div/div[3]/div/div[1]/ul/li')[0]\n",
    "            comment = user_message.text\n",
    "        elif driver.find_elements_by_xpath('//*[@id=\"' + d +'\"]/div/div[3]/div/div[1]'):\n",
    "            user_message = driver.find_elements_by_xpath('//*[@id=\"' + d +'\"]/div/div[3]/div/div[1]')[0]\n",
    "            comment = user_message.text\n",
    "        else: \n",
    "            comment = None\n",
    "        \n",
    "        #Adding date, userid and comment for each user in a dataframe    \n",
    "        all_comments.loc[len(all_comments)] = [date,userid,comment]\n",
    "        \n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_comments.to_csv(\"7500_comments_edmunds.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data\n",
    "We have 7506 rows of comments from 2012-8-10 to 2019-8-21, comtibuted by 130 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"7500_comments_edmunds.csv\", sep=\"\\t\", parse_dates=['Date'])\n",
    "df = df[['Date', 'user_id', 'comments']]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7506, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-08-21 02:25:00')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2012-08-10 01:45:00')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['user_id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\spong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\spong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenization\n",
    "df['tokens'] = df['comments'].map(nltk.word_tokenize)\n",
    "df['tokens'] = df['tokens'].map(lambda x: set(x))\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df ['tokens'].map(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "df['lemmatized'] = df['lemmatized'].map(lambda x: set(x))\n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['processed'] = df['lemmatized'].map(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# remove punctuations\n",
    "punc = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~...\"\n",
    "df['processed'] = df['processed'].map(lambda x: [word for word in x if word.lower() not in punc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some other stuff\n",
    "others = [\"''\", \"``\", \"n't\", \"l\", \"oh\", \"lol\", \"'m\", \"'s\"]\n",
    "df['processed'] = df['processed'].map(lambda x: [word for word in x if word.lower() not in others])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car            3225\n",
       "wa             2298\n",
       "said           2047\n",
       "like           1985\n",
       "would          1894\n",
       "one            1827\n",
       "ha             1719\n",
       "get            1633\n",
       "BMW            1528\n",
       "think          1456\n",
       "new            1414\n",
       "year           1327\n",
       "time           1160\n",
       "even           1108\n",
       "much           1086\n",
       "good           1086\n",
       "know           1065\n",
       "really          992\n",
       "doe             983\n",
       "make            962\n",
       "price           949\n",
       "Audi            946\n",
       "could           935\n",
       "drive           924\n",
       "well            897\n",
       "better          889\n",
       "still           882\n",
       "go              881\n",
       "dealer          863\n",
       "thing           846\n",
       "see             833\n",
       "say             810\n",
       "want            809\n",
       "look            794\n",
       "people          781\n",
       "also            776\n",
       "'ve             775\n",
       "way             773\n",
       "3               741\n",
       "back            715\n",
       "engine          695\n",
       "going           693\n",
       "mile            692\n",
       "lot             687\n",
       "model           686\n",
       "take            651\n",
       "Acura           650\n",
       "need            637\n",
       "'d              632\n",
       "many            628\n",
       "right           617\n",
       "show            612\n",
       "come            603\n",
       "le              597\n",
       "driving         594\n",
       "got             586\n",
       "probably        585\n",
       "sure            580\n",
       "previous        575\n",
       "--              566\n",
       "nice            563\n",
       "something       561\n",
       "used            552\n",
       "buy             546\n",
       "point           537\n",
       "first           526\n",
       "least           512\n",
       "cost            511\n",
       "never           509\n",
       "since           506\n",
       "'ll             506\n",
       "day             504\n",
       "last            502\n",
       "seems           499\n",
       "brand           482\n",
       "great           481\n",
       "issue           480\n",
       "vehicle         478\n",
       "old             473\n",
       "money           470\n",
       "long            470\n",
       "5               466\n",
       "around          465\n",
       "quote           462\n",
       "lease           458\n",
       "may             457\n",
       "might           453\n",
       "pretty          451\n",
       "ca              449\n",
       "2               449\n",
       "course          448\n",
       "little          447\n",
       "looking         445\n",
       "sport           441\n",
       "test            439\n",
       "option          437\n",
       "month           436\n",
       "S4              435\n",
       "'re             429\n",
       "always          428\n",
       "sedan           427\n",
       "work            427\n",
       "4               427\n",
       "find            426\n",
       "every           426\n",
       "though          425\n",
       "feel            424\n",
       "two             424\n",
       "big             423\n",
       "bit             422\n",
       "performance     420\n",
       "seat            417\n",
       "put             417\n",
       "keep            412\n",
       "another         411\n",
       "getting         408\n",
       "TL              402\n",
       "next            401\n",
       "actually        400\n",
       "different       395\n",
       "change          394\n",
       "series          391\n",
       "market          389\n",
       "part            387\n",
       "deal            382\n",
       "TLX             382\n",
       "reason          382\n",
       "problem         381\n",
       "luxury          381\n",
       "Infiniti        380\n",
       "best            379\n",
       "thought         378\n",
       "wife            374\n",
       "wheel           372\n",
       "AWD             370\n",
       "end             370\n",
       "made            370\n",
       "far             367\n",
       "6               361\n",
       "current         360\n",
       "bought          360\n",
       "sale            358\n",
       "use             358\n",
       "high            354\n",
       "number          353\n",
       "interior        351\n",
       "etc             350\n",
       "tire            348\n",
       "power           345\n",
       "couple          345\n",
       "dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "all_words = pd.Series(chain(*list(df['processed'])))\n",
    "all_words.value_counts().head(150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
