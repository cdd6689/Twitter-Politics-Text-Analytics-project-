{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ewarren</td>\n",
       "      <td>Our November fundraising deadline is tonight. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ewarren</td>\n",
       "      <td>I'm proud to have @PaaWeeRivera on my team eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ewarren</td>\n",
       "      <td>Frank LaMere was a dedicated activist who left...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ewarren</td>\n",
       "      <td>On just about every issue we care about—from c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ewarren</td>\n",
       "      <td>Happy #SmallBusinessSaturday! I've got a plan ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  username                                               text\n",
       "0  ewarren  Our November fundraising deadline is tonight. ...\n",
       "1  ewarren  I'm proud to have @PaaWeeRivera on my team eve...\n",
       "2  ewarren  Frank LaMere was a dedicated activist who left...\n",
       "3  ewarren  On just about every issue we care about—from c...\n",
       "4  ewarren  Happy #SmallBusinessSaturday! I've got a plan ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warren = pd.read_csv('ewarren.csv', delimiter=',')\n",
    "df = warren[['username', \"text\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization = split sentences into word strings\n",
    "df['tokens'] = df['text'].map(nltk.word_tokenize)\n",
    "df['tokens'] = df['tokens'].map(lambda x: set(x))\n",
    "\n",
    "# lemmatization = converting a word to its base form, different from stemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df ['tokens'].map(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "df['lemmatized'] = df['lemmatized'].map(lambda x: set(x))\n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['processed'] = df['lemmatized'].map(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# remove punctuations\n",
    "punc = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~...\"\n",
    "df['processed'] = df['processed'].map(lambda x: [word for word in x if word.lower() not in punc])\n",
    "\n",
    "# remove some other stuff and return lower case\n",
    "others = [\"''\", \"``\", \"n't\", \"l\", \"oh\", \"lol\", \"'m\", \"'s\"]\n",
    "df['processed'] = df['processed'].map(lambda x: [word.lower() for word in x if word.lower() not in others])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "…             271\n",
       "’             195\n",
       "fight         100\n",
       "need           85\n",
       "big            82\n",
       "plan           80\n",
       "people         78\n",
       "trump          71\n",
       "got            61\n",
       "make           60\n",
       "family         59\n",
       "ha             59\n",
       "one            58\n",
       "change         54\n",
       "time           53\n",
       "http           50\n",
       "fighting       49\n",
       "get            49\n",
       "'ll            47\n",
       "everyone       46\n",
       "'ve            46\n",
       "president      46\n",
       "structural     45\n",
       "must           45\n",
       "end            43\n",
       "every          42\n",
       "right          42\n",
       "government     42\n",
       "today          36\n",
       "donald         36\n",
       "care           36\n",
       "like           35\n",
       "going          34\n",
       "'re            33\n",
       "work           33\n",
       "wa             32\n",
       "facebook       32\n",
       "corruption     32\n",
       "climate        32\n",
       "year           32\n",
       "public         31\n",
       "american       31\n",
       "protect        31\n",
       "power          31\n",
       "together       30\n",
       "let            30\n",
       "black          30\n",
       "working        29\n",
       "health         29\n",
       "want           28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = pd.Series(chain(*list(df['processed']))) \n",
    "word_frequency = all_words.value_counts()\n",
    "word_frequency[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "’            26\n",
       "—            19\n",
       "president    17\n",
       "http         14\n",
       "trump        14\n",
       "…            12\n",
       "donald       10\n",
       "day           8\n",
       "build         7\n",
       "need          7\n",
       "inclusive     6\n",
       "take          6\n",
       "middle        6\n",
       "ha            6\n",
       "one           6\n",
       "ensure        6\n",
       "ready         6\n",
       "america       6\n",
       "time          6\n",
       "american      6\n",
       "country       5\n",
       "climate       5\n",
       "iowa          5\n",
       "'ll           5\n",
       "violence      5\n",
       "world         5\n",
       "plan          5\n",
       "every         5\n",
       "class         5\n",
       "must          4\n",
       "year          4\n",
       "safe          4\n",
       "americans     4\n",
       "white         4\n",
       "protect       4\n",
       "change        4\n",
       "folk          4\n",
       "community     4\n",
       "chip          4\n",
       "full          4\n",
       "read          4\n",
       "work          4\n",
       "win           3\n",
       "right         3\n",
       "family        3\n",
       "new           3\n",
       "tax           3\n",
       "come          3\n",
       "power         3\n",
       "action        3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warren = pd.read_csv('JoeBiden.csv', delimiter=',')\n",
    "df = warren[['username', \"text\"]]\n",
    "\n",
    "# tokenization = split sentences into word strings\n",
    "df['tokens'] = df['text'].map(nltk.word_tokenize)\n",
    "df['tokens'] = df['tokens'].map(lambda x: set(x))\n",
    "\n",
    "# lemmatization = converting a word to its base form, different from stemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df ['tokens'].map(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "df['lemmatized'] = df['lemmatized'].map(lambda x: set(x))\n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['processed'] = df['lemmatized'].map(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# remove punctuations\n",
    "punc = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~...\"\n",
    "df['processed'] = df['processed'].map(lambda x: [word for word in x if word.lower() not in punc])\n",
    "\n",
    "# remove some other stuff and return lower case\n",
    "others = [\"''\", \"``\", \"n't\", \"l\", \"oh\", \"lol\", \"'m\", \"'s\"]\n",
    "df['processed'] = df['processed'].map(lambda x: [word.lower() for word in x if word.lower() not in others])\n",
    "\n",
    "all_words = pd.Series(chain(*list(df['processed']))) \n",
    "word_frequency = all_words.value_counts()\n",
    "word_frequency[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36workshop",
   "language": "python",
   "name": "p36workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
